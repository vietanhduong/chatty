[general]
# Default welcome message
hello_message = "Hello! How can I help you? ðŸ˜Š"
# Show usage statistics when ever a chat completion completed
show_usage = false
# Verbose info in starting up. This different from the log level
# and will only print in the beginning to stderr.
verbose = false
# The percentage of the screen width to use for the bubble width
# Can't be more than 95% and less than 50%
bubble_width_percent = 60
# If set, the initialize screen will auto close after the inititalization
# is done.
auto_start = false

[log]
# Default log level is "info"
level = "info"

# Set log level for specific modules
# e.g: filters = [{ module = "chatty::backend", level = "trace" }]
filters = []

[log.file]
# You can use env var to set the path e.g: $HOME/chatty.log
path = "/tmp/chatty.log"
append = false

# Theme settings
[theme]
name = "base16-ocean.dark"
folder_path = ""

[context.compression]
enabled = false
## Specify the compression model to use. The model must be available in the
## backend. If not set, the current conversation model will be used.
compress_model = ""
## The maximum number of tokens in the context before trigger compression
max_tokens = 65_536
## The maximum number of messages in conversation before trigger compression
max_messages = 50
## The number of latest messages to keep in the context
keep_n_messages = 10

[context.truncation]
enabled = false
## If the total no. tokens in the current context is greater than
## context.truncation.max_tokens + backend.connections.max_output_tokens
## then the context will be truncated to fit the limit
max_tokens = 65_536

[backend]
## Default backend model to use. If not set or not present in the models list
## then the first model in the list will be used.
default_model = ""

## Backend model settings
#[[backend.model_settings]]
#model.contains = "" # One of the following: "contains", "equals", "regex" must be set
#model.equals = ""
#model.regex = ""
#enable_mcp = true # Default is true

## At least one backend connection must be enabled
#[[backend.connections]]
#enabled = false
#alias = "OpenAI"
#kind = "openai"    # openai or gemini
#max_output_tokens = None
#endpoint = ""
#api_key = ""

## Backend MCP servers
## Support for multiple servers. Only 'binary' and 'websocket' are supported
## NOTES:
## - If tool name is conflicted, the longest one will be used.
#[backend.mcp]
## If set, chatty will send a notice when ever a tool is called
#notice_on_call_tool = false

#[[backend.mcp.servers]]
#enabled = true # Default is true
#binary.filename = "" # Path to the binary file
#binary.args = [] # Arguments to pass to the binary
#binary.env = {} # Environment variables to set for the binary

#[[backend.mcp_servers]]
#websocket.url = ""


[storage.sqlite]
# Leave it empty to use in-memory database
#path = ""
